{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/15 15:21:59 WARN Utils: Your hostname, Halils-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.51.47.94 instead (on interface en0)\n",
      "23/12/15 15:21:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/15 15:22:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "data_pd = pd.read_csv('/Users/halilergul/Desktop/master/fall-23_24/leaf-dataset/leaf.csv', header=None)\n",
    "# column names are like numbers 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14 ,15 \n",
    "# I want to give them features names: feature1, etc.\n",
    "data_pd.columns = ['class', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8',\n",
    "                'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16']\n",
    "spark = SparkSession.builder.appName(\"LeafClassification\").getOrCreate()\n",
    "data = spark.createDataFrame(data_pd) # by this, I convert pandas dataframe to spark dataframe\n",
    "# Vectorizing features\n",
    "columns = data.columns # result is: ['class', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16']\n",
    "assembler = VectorAssembler(inputCols=columns[1:], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)# Split the data into training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/15 15:22:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                         |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.0,0.72694,1.4742,0.32396,0.98535,1.0,0.83592,0.0046566,0.0039465,0.04779,0.12795,0.016108,0.0052323,2.7477E-4,1.1756]         |\n",
      "|[2.0,0.74173,1.5257,0.36116,0.98152,0.99825,0.79867,0.0052423,0.0050016,0.02416,0.090476,0.0081195,0.002708,7.4846E-5,0.69659]   |\n",
      "|[3.0,0.76722,1.5725,0.38998,0.97755,1.0,0.80812,0.0074573,0.010121,0.011897,0.057445,0.0032891,9.2068E-4,3.7886E-5,0.44348]      |\n",
      "|[4.0,0.73797,1.4597,0.35376,0.97566,1.0,0.81697,0.0068768,0.0086068,0.01595,0.065491,0.0042707,0.0011544,6.6272E-5,0.58785]      |\n",
      "|[5.0,0.82301,1.7707,0.44462,0.97698,1.0,0.75493,0.007428,0.010042,0.0079379,0.045339,0.0020514,5.5986E-4,2.3504E-5,0.34214]      |\n",
      "|[6.0,0.72997,1.4892,0.34284,0.98755,1.0,0.84482,0.0049451,0.0044506,0.010487,0.058528,0.0034138,0.0011248,2.4798E-5,0.34068]     |\n",
      "|[7.0,0.82063,1.7529,0.44458,0.97964,0.99649,0.7677,0.0059279,0.0063954,0.018375,0.080587,0.0064523,0.0022713,4.1495E-5,0.53904]  |\n",
      "|[8.0,0.77982,1.6215,0.39222,0.98512,0.99825,0.80816,0.0050987,0.0047314,0.024875,0.089686,0.0079794,0.0024664,1.4676E-4,0.66975] |\n",
      "|[9.0,0.83089,1.8199,0.45693,0.9824,1.0,0.77106,0.0060055,0.006564,0.0072447,0.040616,0.0016469,3.8812E-4,3.2863E-5,0.33696]      |\n",
      "|[10.0,0.90631,2.3906,0.58336,0.97683,0.99825,0.66419,0.0084019,0.012848,0.0070096,0.042347,0.0017901,4.5889E-4,2.8251E-5,0.28082]|\n",
      "|[11.0,0.7459,1.4927,0.34116,0.98296,1.0,0.83088,0.0055665,0.0056395,0.0057679,0.036511,0.0013313,3.0872E-4,3.1839E-5,0.25026]    |\n",
      "|[12.0,0.79606,1.6934,0.43387,0.98181,1.0,0.76985,0.0077992,0.011071,0.013677,0.057832,0.0033334,8.1648E-4,1.3855E-4,0.49751]     |\n",
      "|[1.0,0.93361,2.7582,0.64257,0.98346,1.0,0.59851,0.0055336,0.0055731,0.029712,0.089889,0.0080153,0.0020648,2.3883E-4,0.91499]     |\n",
      "|[2.0,0.91186,2.4994,0.60323,0.983,1.0,0.64916,0.0061494,0.0068823,0.018887,0.072486,0.0052267,0.0014887,8.3271E-5,0.67811]       |\n",
      "|[3.0,0.89063,2.2927,0.56667,0.98732,1.0,0.66427,0.0028365,0.0014643,0.029272,0.091328,0.0082717,0.0022383,2.0166E-4,0.87177]     |\n",
      "|[4.0,0.86755,2.009,0.51464,0.98691,1.0,0.70277,0.0054439,0.0053937,0.030348,0.092063,0.0084044,0.0022541,1.9854E-4,0.94545]      |\n",
      "|[5.0,0.91852,2.5247,0.61648,0.9787,1.0,0.63037,0.0050494,0.0046404,0.02309,0.082029,0.0066839,0.0018929,1.2452E-4,0.71713]       |\n",
      "|[6.0,0.88795,2.2038,0.56218,0.97835,0.99825,0.64158,0.0059242,0.0063874,0.032722,0.092969,0.0085691,0.0021199,2.7729E-4,1.008]   |\n",
      "|[7.0,0.85121,1.9548,0.4892,0.98622,1.0,0.70267,0.0039733,0.0028733,0.020258,0.070841,0.0049933,0.0012274,1.4929E-4,0.74174]      |\n",
      "|[8.0,0.89084,2.2979,0.57815,0.97389,1.0,0.64598,0.015271,0.042443,0.028461,0.086477,0.0074228,0.0018832,2.4345E-4,0.91307]       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# see whether assembler took the right columns\n",
    "data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.69\n",
      "Best model parameters: numTrees: 60, maxDepth: <bound method _DecisionTreeParams.getMaxDepth of RandomForestClassificationModel: uid=RandomForestClassifier_d4346c77ae8e, numTrees=60, numClasses=37, numFeatures=15>\n"
     ]
    }
   ],
   "source": [
    "spark_context = spark.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\") # I do this to see only errors, not warnings\n",
    "rf = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\") #Â classifier\n",
    "#parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 15, 20, 25, 30, 40, 50, 60, 70]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15, 20, 25, 30]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\") # multiclass evaluator\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train_data) # this will run the classifier with all the parameters in the grid\n",
    "predictions = cvModel.transform(test_data) #the best parameters to make predictions on the test set\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy)) # also print best models parameters\n",
    "print(\"Best model parameters: numTrees: {}, maxDepth: {}\".format(cvModel.bestModel.getNumTrees,\n",
    "                                                                 cvModel.bestModel.getMaxDepth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6789891046771781\n",
      "Best model parameters: regParam: 0.01, maxIter: 20, elasticNetParam: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Leaf Classification\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Now I will define a schema to load the data and give the columns names as feature1, feature2, etc.\n",
    "schema = StructType([StructField(\"label\", IntegerType(), True)])\n",
    "for i in range(1, 16):\n",
    "    schema.add(StructField(f\"feature{i}\", DoubleType(), True))\n",
    "data = spark.read.csv(\"leaf.csv\", schema=schema, header=False) # this is my data\n",
    "\n",
    "feature_columns = data.columns[1:]  # excluding the first column which is the label\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# now classifier\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train_data)\n",
    "prediction = cvModel.transform(test_data)\n",
    "\n",
    "#now model evaluation\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "print(\"Best model parameters: regParam: {}, maxIter: {}, elasticNetParam: {}\".format(cvModel.bestModel.getRegParam(),\n",
    "                                                                                     cvModel.bestModel.getMaxIter(),\n",
    "                                                                                     cvModel.bestModel.getElasticNetParam()))\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5118523581681477\n",
      "Best model parameters: maxDepth: 15, maxBins: 40\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Leaf Classification\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "# I use schema logic to define the columns names similar to previous examples\n",
    "schema = StructType([StructField(\"label\", IntegerType(), True)])\n",
    "for i in range(1, 16):\n",
    "    schema.add(StructField(f\"feature{i}\", DoubleType(), True))\n",
    "\n",
    "data = spark.read.csv(\"leaf.csv\", schema=schema, header=False)\n",
    "\n",
    "feature_columns = data.columns[1:]  # excluding the first column which is the label\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# add a lot of maxDepth and maxBins parameters to the grid to get the best model\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20, 25, 30]) \\\n",
    "    .addGrid(dt.maxBins, [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]) \\\n",
    "    .build()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\") # usual evaluator as multiclass\n",
    "\n",
    "# CrossValidator\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train_data)\n",
    "prediction = cvModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "print(\"Best model parameters: maxDepth: {}, maxBins: {}\".format(cvModel.bestModel.getMaxDepth(),\n",
    "                                                                cvModel.bestModel.getMaxBins()))\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-Rest classifier (a.k.a. One-vs-All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.7856254856254857\n",
      "Best model parameters: maxIter: 10, regParam: 0.001\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Leaf Classification\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([StructField(\"label\", IntegerType(), True)])\n",
    "for i in range(1, 16):\n",
    "    schema.add(StructField(f\"feature{i}\", DoubleType(), True))\n",
    "\n",
    "data = spark.read.csv(\"leaf.csv\", schema=schema, header=False)\n",
    "\n",
    "feature_columns = data.columns[1:]  # excluding the first column which is the label\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "ovr = OneVsRest(classifier=lr, featuresCol=\"features\", labelCol=\"label\") # this is the one vs rest classifier from spark\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [10, 100, 200]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .build()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\") # the same evaluator as before\n",
    "crossval = CrossValidator(estimator=ovr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "prediction = cvModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "bestModel = cvModel.bestModel.getClassifier()\n",
    "print(\"Best model parameters: maxIter: {}, regParam: {}\".format(bestModel.getMaxIter(),\n",
    "                                                                bestModel.getRegParam()))\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>numTrees: 25, maxDepth: 5</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>regParam: 0.01, maxIter: 20, elasticNetParam: 0.0</td>\n",
       "      <td>0.6790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>maxDepth: 15, maxBins: 40</td>\n",
       "      <td>0.5939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One-vs-Rest Classifier</td>\n",
       "      <td>maxIter: 10, regParam: 0.001</td>\n",
       "      <td>0.7856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Method  \\\n",
       "0  Random Forest Classifier   \n",
       "1       Logistic Regression   \n",
       "2  Decision Tree Classifier   \n",
       "3    One-vs-Rest Classifier   \n",
       "\n",
       "                                          Parameters  Accuracy  \n",
       "0                          numTrees: 25, maxDepth: 5    0.6700  \n",
       "1  regParam: 0.01, maxIter: 20, elasticNetParam: 0.0    0.6790  \n",
       "2                          maxDepth: 15, maxBins: 40    0.5939  \n",
       "3                       maxIter: 10, regParam: 0.001    0.7856  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to hold the classification results\n",
    "classification_results = pd.DataFrame({\n",
    "    \"Method\": [\n",
    "        \"Random Forest Classifier\",\n",
    "        \"Logistic Regression\",\n",
    "        \"Decision Tree Classifier\",\n",
    "        \"One-vs-Rest Classifier\"\n",
    "    ],\n",
    "    \"Parameters\": [\n",
    "        \"numTrees: 25, maxDepth: 5\",\n",
    "        \"regParam: 0.01, maxIter: 20, elasticNetParam: 0.0\",\n",
    "        \"maxDepth: 15, maxBins: 40\",\n",
    "        \"maxIter: 10, regParam: 0.001\"\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        0.67,\n",
    "        0.6789891046771781,\n",
    "        0.5938716227707053,\n",
    "        0.7856254856254857\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Set the display format for the accuracy to be more readable\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "# Print out the DataFrame\n",
    "classification_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
