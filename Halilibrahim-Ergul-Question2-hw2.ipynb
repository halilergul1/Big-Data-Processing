{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To make sure and compare, I used and generated different similarity metrics and similarty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=27531Kb max_used=27567Kb free=103540Kb\n",
      " bounds [0x00000001081e0000, 0x0000000109d00000, 0x00000001101e0000]\n",
      " total_blobs=10603 nmethods=9635 adapters=879\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+\n",
      "|user1|user2| jaccard_similarity|\n",
      "+-----+-----+-------------------+\n",
      "|    1|  313|0.23010752688172043|\n",
      "|    1|  330|0.20351758793969849|\n",
      "|    1|  452|0.19889502762430938|\n",
      "|    1|  266|0.19420289855072465|\n",
      "|    1|   45|0.18832391713747645|\n",
      "|    1|   57|0.18791946308724833|\n",
      "|    1|  469|0.18739352640545145|\n",
      "|    1|  577|0.18731117824773413|\n",
      "|    1|  135|0.18561484918793503|\n",
      "|    1|   39|0.18149466192170818|\n",
      "|    2|  366|0.17647058823529413|\n",
      "|    2|  378|0.16666666666666666|\n",
      "|    2|  417|0.14285714285714285|\n",
      "|    2|  461|0.14285714285714285|\n",
      "|    2|  550|               0.14|\n",
      "|    2|  189|0.13953488372093023|\n",
      "|    2|  433|0.13333333333333333|\n",
      "|    2|  435|0.12698412698412698|\n",
      "|    2|   65|              0.125|\n",
      "|    2|  209|0.12280701754385964|\n",
      "+-----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_set, size, array_distinct, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "spark = SparkSession.builder.appName(\"JaccardSimilarity\").getOrCreate()\n",
    "ratings_df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "movies_per_user_df = ratings_df.groupBy(\"userId\").agg(collect_set(\"movieId\").alias(\"movies\")) # Here I create a list of movies rated by each user\n",
    "user_pairs_df = movies_per_user_df.alias(\"u1\").join(movies_per_user_df.alias(\"u2\"), col(\"u1.userId\") < col(\"u2.userId\")) # Join users with themselves to get all possible pairs\n",
    "# I will calculate intersection and union now to find Jaccard Similarity\n",
    "user_pairs_df = user_pairs_df.withColumn(\"intersection\", size(expr(\"array_intersect(u1.movies, u2.movies)\"))) \\\n",
    "                             .withColumn(\"union\", size(expr(\"array_distinct(concat(u1.movies, u2.movies))\"))) # I concat the two lists and find the distinct elements to get the union\n",
    "user_pairs_df = user_pairs_df.withColumn(\"jaccard_similarity\", col(\"intersection\") / col(\"union\"))\n",
    "user_pairs_df = user_pairs_df.filter(col(\"jaccard_similarity\") > 0).select(col(\"u1.userId\").alias(\"user1\"), col(\"u2.userId\").alias(\"user2\"), \"jaccard_similarity\") # Filter out users with 0 similarity\n",
    "# Finding top 10 \n",
    "window = Window.partitionBy(\"user1\").orderBy(col(\"jaccard_similarity\").desc())\n",
    "top_10_similar_users = user_pairs_df.withColumn(\"rank\", rank().over(window)) \\\n",
    "                                    .filter(col(\"rank\") <= 10) \\\n",
    "                                    .select(\"user1\", \"user2\", \"jaccard_similarity\")\n",
    "top_10_similar_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pearson_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+\n",
      "|user1|user2|pearson_correlation|\n",
      "+-----+-----+-------------------+\n",
      "|    1|   77| 1.0000000000000002|\n",
      "|    1|   13| 0.9478788458420679|\n",
      "|    1|  157|  0.901774575834698|\n",
      "|    1|  139| 0.8903416876712336|\n",
      "|    1|  401| 0.8713212630061857|\n",
      "|    1|  511| 0.8655816064894617|\n",
      "|    1|  473|  0.840746829564101|\n",
      "|    1|  366| 0.8352756122978426|\n",
      "|    1|  258| 0.8320502943378436|\n",
      "|    1|   65| 0.8235706673737208|\n",
      "|    2|  246| 0.7901797012290985|\n",
      "|    2|   91| 0.7173712228393851|\n",
      "|    2|  189| 0.6281794253820807|\n",
      "|    2|  332| 0.5764940711934559|\n",
      "|    2|  326| 0.5539173064473961|\n",
      "|    2|  393| 0.5480433465368264|\n",
      "|    2|  308| 0.5123980142682039|\n",
      "|    2|  596|  0.492892933147424|\n",
      "|    2|  209| 0.4904972228097824|\n",
      "|    2|  567|0.48496919806182326|\n",
      "+-----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.functions import sqrt\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PearsonCorrelation\").getOrCreate()\n",
    "ratings_df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "mean_ratings_df = ratings_df.groupBy(\"userId\").agg(avg(\"rating\").alias(\"mean_rating\")) # I calculate mean ratings for each user\n",
    "ratings_with_mean_df = ratings_df.join(mean_ratings_df, \"userId\") # I join mean ratings with ratings dataframe to get mean ratings for each rating\n",
    "# Now I will calculate Pearson Correlation Coefficient for each pair of users.\n",
    "pairs_df = ratings_with_mean_df.alias(\"r1\").join(ratings_with_mean_df.alias(\"r2\"), col(\"r1.movieId\") == col(\"r2.movieId\")) \\\n",
    "                               .filter(col(\"r1.userId\") < col(\"r2.userId\"))\n",
    "stats_df = pairs_df.groupBy(\"r1.userId\", \"r2.userId\").agg(\n",
    "    count(col(\"r1.movieId\")).alias(\"num_common_movies\"),\n",
    "    sum((col(\"r1.rating\") - col(\"r1.mean_rating\")) * (col(\"r2.rating\") - col(\"r2.mean_rating\"))).alias(\"dot_product\"),\n",
    "    sum((col(\"r1.rating\") - col(\"r1.mean_rating\"))**2).alias(\"rating_sq_sum_user1\"),\n",
    "    sum((col(\"r2.rating\") - col(\"r2.mean_rating\"))**2).alias(\"rating_sq_sum_user2\")\n",
    ")\n",
    "pearson_df = stats_df.withColumn(\"pearson_correlation\", col(\"dot_product\") / (sqrt(\"rating_sq_sum_user1\") * sqrt(\"rating_sq_sum_user2\")))\n",
    "filtered_pearson_df = pearson_df.filter((col(\"pearson_correlation\") > 0) & (col(\"num_common_movies\") > 3)) # here I filter out users with 0 correlation and less than 3 common movies\n",
    "# Finding top 10 \n",
    "window = Window.partitionBy(\"r1.userId\").orderBy(col(\"pearson_correlation\").desc())\n",
    "top_10_similar_users = filtered_pearson_df.withColumn(\"rank\", rank().over(window)) \\\n",
    "                                          .filter(col(\"rank\") <= 10) \\\n",
    "                                          .select(col(\"r1.userId\").alias(\"user1\"), col(\"r2.userId\").alias(\"user2\"), \"pearson_correlation\")\n",
    "top_10_similar_users.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adjusted_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/25 16:56:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/25 16:56:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23510Kb max_used=23510Kb free=107561Kb\n",
      " bounds [0x000000010c1e0000, 0x000000010d900000, 0x00000001141e0000]\n",
      " total_blobs=9464 nmethods=8511 adapters=862\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------------+\n",
      "|user1|user2|adjusted_cosine_similarity|\n",
      "+-----+-----+--------------------------+\n",
      "|    1|   77|        1.0000000000000002|\n",
      "|    1|  291|                       1.0|\n",
      "|    1|  358|                       1.0|\n",
      "|    1|   85|                       1.0|\n",
      "|    1|  388|                       1.0|\n",
      "|    1|   12|                       1.0|\n",
      "|    1|  253|                       1.0|\n",
      "|    1|    2|        0.9999999999999998|\n",
      "|    1|  146|        0.9990496408681655|\n",
      "|    1|  278|        0.9710607611177227|\n",
      "|    2|  333|                       1.0|\n",
      "|    2|  299|                       1.0|\n",
      "|    2|  426|                       1.0|\n",
      "|    2|  213|                       1.0|\n",
      "|    2|  267|                       1.0|\n",
      "|    2|  563|                       1.0|\n",
      "|    2|  225|                       1.0|\n",
      "|    2|  174|                       1.0|\n",
      "|    2|  101|                       1.0|\n",
      "|    2|  216|                       1.0|\n",
      "+-----+-----+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, sqrt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "spark = SparkSession.builder.appName(\"AdjustedCosineSimilarity\").getOrCreate()\n",
    "ratings_df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "avg_ratings_df = ratings_df.groupBy(\"userId\").agg(avg(\"rating\").alias(\"avg_rating\")) # I calculate mean ratings for each user\n",
    "normalized_ratings_df = ratings_df.join(avg_ratings_df, \"userId\").withColumn(\"norm_rating\", col(\"rating\") - col(\"avg_rating\")) # I join mean ratings with ratings dataframe to get mean ratings for each rating\n",
    "\n",
    "user_pairs_df = normalized_ratings_df.alias(\"r1\").join(normalized_ratings_df.alias(\"r2\"), col(\"r1.movieId\") == col(\"r2.movieId\")) \\\n",
    "                                     .filter(col(\"r1.userId\") < col(\"r2.userId\"))\n",
    "# Now I will calculate Adjusted Cosine Similarity for each pair of users.\n",
    "adj_cosine_components_df = user_pairs_df.groupBy(\"r1.userId\", \"r2.userId\").agg(\n",
    "    sum(col(\"r1.norm_rating\") * col(\"r2.norm_rating\")).alias(\"dot_product\"),\n",
    "    sqrt(sum(col(\"r1.norm_rating\")**2)).alias(\"norm_user1\"),\n",
    "    sqrt(sum(col(\"r2.norm_rating\")**2)).alias(\"norm_user2\")\n",
    ")\n",
    "adj_cosine_similarity_df = adj_cosine_components_df.withColumn(\"adjusted_cosine_similarity\", col(\"dot_product\") / (col(\"norm_user1\") * col(\"norm_user2\")))\n",
    "\n",
    "# Finding top 10\n",
    "window = Window.partitionBy(\"r1.userId\").orderBy(col(\"adjusted_cosine_similarity\").desc())\n",
    "top_10_similar_users = adj_cosine_similarity_df.withColumn(\"rank\", rank().over(window)) \\\n",
    "                                                .filter(col(\"rank\") <= 10) \\\n",
    "                                                .select(col(\"r1.userId\").alias(\"user1\"), col(\"r2.userId\").alias(\"user2\"), \"adjusted_cosine_similarity\")\n",
    "\n",
    "top_10_similar_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# euclidean_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 13:20:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/26 13:20:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23769Kb max_used=24037Kb free=107302Kb\n",
      " bounds [0x000000010a1e0000, 0x000000010b980000, 0x00000001121e0000]\n",
      " total_blobs=9507 nmethods=8554 adapters=862\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|user1|user2|euclidean_similarity|\n",
      "+-----+-----+--------------------+\n",
      "|    1|  253|   0.984915545117876|\n",
      "|    1|  291|  0.9022707314013297|\n",
      "|    1|  358|  0.9006722848215131|\n",
      "|    1|  388|  0.7972508591065292|\n",
      "|    1|  472|  0.7387674456990245|\n",
      "|    1|  550|  0.6951179407743328|\n",
      "|    1|   85|    0.60222934799206|\n",
      "|    1|  278|  0.5899813771892649|\n",
      "|    1|  366|  0.5785567963282364|\n",
      "|    1|   12|  0.5742521316444738|\n",
      "|    2|  168|  0.9857168685590311|\n",
      "|    2|  369|  0.9462501580877704|\n",
      "|    2|   96|  0.9401496259351623|\n",
      "|    2|  329|  0.9270326615705352|\n",
      "|    2|  368|  0.9041114102436268|\n",
      "|    2|  213|  0.8921442959164991|\n",
      "|    2|  172|  0.8917800118273215|\n",
      "|    2|  186|  0.8838840188806469|\n",
      "|    2|  287|  0.8521991300144997|\n",
      "|    2|  291|  0.8401869158878504|\n",
      "+-----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sqrt, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EuclideanSimilarity\").getOrCreate()\n",
    "ratings_df = spark.read.csv(\"ratings.csv\", header=True, inferSchema=True)\n",
    "avg_ratings_df = ratings_df.groupBy(\"userId\").agg(avg(\"rating\").alias(\"avg_rating\")) # I calculate mean ratings for each user\n",
    "normalized_ratings_df = ratings_df.join(avg_ratings_df, \"userId\").withColumn(\"norm_rating\", col(\"rating\") - col(\"avg_rating\")) # I join mean ratings with ratings dataframe to get mean ratings for each rating\n",
    "# Now I will create pairs of users who have rated the same movie\n",
    "user_pairs_df = normalized_ratings_df.alias(\"r1\").join(normalized_ratings_df.alias(\"r2\"), col(\"r1.movieId\") == col(\"r2.movieId\")) \\\n",
    "                                     .filter(col(\"r1.userId\") < col(\"r2.userId\"))\n",
    "#Euclidean Distance\n",
    "euclidean_components_df = user_pairs_df.groupBy(\"r1.userId\", \"r2.userId\").agg(\n",
    "    spark_sum((col(\"r1.norm_rating\") - col(\"r2.norm_rating\"))**2).alias(\"squared_distance\")\n",
    ")\n",
    "# Now, I need to convert this to similarity. I will use 1 / (1 + distance) to convert distance to similarity. Inverted Euclidean Distance.\n",
    "euclidean_similarity_df = euclidean_components_df.withColumn(\"euclidean_similarity\", 1 / (sqrt(\"squared_distance\") + 1))  # Add 1 to avoid division by zero\n",
    "# Finding top 10\n",
    "window = Window.partitionBy(\"r1.userId\").orderBy(col(\"euclidean_similarity\").desc())\n",
    "top_10_similar_users = euclidean_similarity_df.withColumn(\"rank\", rank().over(window)) \\\n",
    "                                              .filter(col(\"rank\") <= 10) \\\n",
    "                                              .select(col(\"r1.userId\").alias(\"user1\"), col(\"r2.userId\").alias(\"user2\"), \"euclidean_similarity\")\n",
    "top_10_similar_users.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
